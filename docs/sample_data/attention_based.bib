@article{eaaf53202a815c6f6dbafddf1e873778921da90e,
title = {A self-attention based message passing neural network for predicting molecular lipophilicity and aqueous solubility},
year = {2020},
url = {https://www.semanticscholar.org/paper/eaaf53202a815c6f6dbafddf1e873778921da90e},
abstract = {S2 TL;DR: A graph-neural-network framework called self-attention-based message-passing neural network (SAMPN) is built and applied to study the relationship between chemical properties and structures in an interpretable way and can generate chemically visible and interpretable results, which can help researchers discover new pharmaceuticals and materials.},
author = {Bowen Tang and Skyler T. Kramer and Meijuan Fang and Yingkun Qiu and Zhen Wu and Dong Xu},
journal = {Journal of Cheminformatics},
volume = {12},
pages = {null},
doi = {10.1186/s13321-020-0414-z},
pmid = {33431047},
}

@article{9c575c388947e9ff37f6023853c83421c1a3614b,
title = {Comparison of Atom Representations in Graph Neural Networks for Molecular Property Prediction},
year = {2020},
url = {https://www.semanticscholar.org/paper/9c575c388947e9ff37f6023853c83421c1a3614b},
abstract = {Graph neural networks have recently become a standard method for analysing chemical compounds. In the field of molecular property prediction, the emphasis is now put on designing new model architectures, and the importance of atom featurisation is oftentimes belittled. When contrasting two graph neural networks, the use of different atom features possibly leads to the incorrect attribution of the results to the network architecture. To provide a better understanding of this issue, we compare multiple atom representations for graph models and evaluate them on the prediction of free energy, solubility, and metabolic stability. To the best of our knowledge, this is the first methodological study that focuses on the relevance of atom representation to the predictive performance of graph neural networks.},
author = {Agnieszka Pocha and Tomasz Danel and Lukasz Maziarka},
journal = {2021 International Joint Conference on Neural Networks (IJCNN)},
volume = {null},
pages = {1-8},
doi = {10.1109/IJCNN52387.2021.9533698},
arxivid = {2012.04444},
}
